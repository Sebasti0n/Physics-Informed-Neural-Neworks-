{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import argparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torchopt\n",
    "\n",
    "from pinn import make_forward_fn, LinearNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 1.0  # rate of maximum population growth parameterizing the equation\n",
    "X_BOUNDARY = 0.0  # boundary condition coordinate\n",
    "F_BOUNDARY = 0.5  # boundary condition value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loss_fn(f: Callable, dfdx: Callable) -> Callable:\n",
    "    \"\"\"Make a function loss evaluation function\n",
    "\n",
    "    The loss is computed as sum of the interior MSE loss (the differential equation residual)\n",
    "    and the MSE of the loss at the boundary\n",
    "\n",
    "    Args:\n",
    "        f (Callable): The functional forward pass of the model used a universal function approximator. This\n",
    "            is a function with signature (x, params) where `x` is the input data and `params` the model\n",
    "            parameters\n",
    "        dfdx (Callable): The functional gradient calculation of the universal function approximator. This\n",
    "            is a function with signature (x, params) where `x` is the input data and `params` the model\n",
    "            parameters\n",
    "\n",
    "    Returns:\n",
    "        Callable: The loss function with signature (params, x) where `x` is the input data and `params` the model\n",
    "            parameters. Notice that a simple call to `dloss = functorch.grad(loss_fn)` would give the gradient\n",
    "            of the loss with respect to the model parameters needed by the optimizers\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_fn(params: torch.Tensor, x: torch.Tensor):\n",
    "\n",
    "        # interior loss\n",
    "        f_value = f(x, params)\n",
    "        interior = dfdx(x, params) - R * f_value * (1 - f_value)\n",
    "\n",
    "        # boundary loss\n",
    "        x0 = X_BOUNDARY\n",
    "        f0 = F_BOUNDARY\n",
    "        x_boundary = torch.tensor([x0])\n",
    "        f_boundary = torch.tensor([f0])\n",
    "        boundary = f(x_boundary, params) - f_boundary\n",
    "\n",
    "        loss = nn.MSELoss()\n",
    "        loss_value = loss(interior, torch.zeros_like(interior)) + loss(\n",
    "            boundary, torch.zeros_like(boundary)\n",
    "        )\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # make it reproducible\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # parse input from user\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"-n\", \"--num-hidden\", type=int, default=5)\n",
    "    parser.add_argument(\"-d\", \"--dim-hidden\", type=int, default=5)\n",
    "    parser.add_argument(\"-b\", \"--batch-size\", type=int, default=30)\n",
    "    parser.add_argument(\"-lr\", \"--learning-rate\", type=float, default=1e-1)\n",
    "    parser.add_argument(\"-e\", \"--num-epochs\", type=int, default=100)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # configuration\n",
    "    num_hidden = args.num_hidden\n",
    "    dim_hidden = args.dim_hidden\n",
    "    batch_size = args.batch_size\n",
    "    num_iter = args.num_epochs\n",
    "    tolerance = 1e-8\n",
    "    learning_rate = args.learning_rate\n",
    "    domain = (-5.0, 5.0)\n",
    "\n",
    "    # function versions of model forward, gradient and loss\n",
    "    model = LinearNN(num_layers=num_hidden, num_neurons=dim_hidden, num_inputs=1)\n",
    "    funcs = make_forward_fn(model, derivative_order=1)\n",
    "\n",
    "    f = funcs[0]\n",
    "    dfdx = funcs[1]\n",
    "    loss_fn = make_loss_fn(f, dfdx)\n",
    "\n",
    "    # choose optimizer with functional API using functorch\n",
    "    optimizer = torchopt.FuncOptimizer(torchopt.adam(lr=learning_rate))\n",
    "\n",
    "    # initial parameters randomly initialized\n",
    "    params = tuple(model.parameters())\n",
    "\n",
    "    # train the model\n",
    "    loss_evolution = []\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        # sample points in the domain randomly for each epoch\n",
    "        x = torch.FloatTensor(batch_size).uniform_(domain[0], domain[1])\n",
    "\n",
    "        # compute the loss with the current parameters\n",
    "        loss = loss_fn(params, x)\n",
    "\n",
    "        # update the parameters with functional optimizer\n",
    "        params = optimizer.step(loss, params)\n",
    "\n",
    "        print(f\"Iteration {i} with loss {float(loss)}\")\n",
    "        loss_evolution.append(float(loss))\n",
    "\n",
    "    # plot solution on the given domain\n",
    "    x_eval = torch.linspace(domain[0], domain[1], steps=100).reshape(-1, 1)\n",
    "    f_eval = f(x_eval, params)\n",
    "    analytical_sol_fn = lambda x: 1.0 / (1.0 + (1.0/F_BOUNDARY - 1.0) * np.exp(-R * x))\n",
    "    x_eval_np = x_eval.detach().numpy()\n",
    "    x_sample_np = torch.FloatTensor(batch_size).uniform_(domain[0], domain[1]).detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.scatter(x_sample_np, analytical_sol_fn(x_sample_np), color=\"red\", label=\"Sample training points\")\n",
    "    ax.plot(x_eval_np, f_eval.detach().numpy(), label=\"PINN final solution\")\n",
    "    ax.plot(\n",
    "        x_eval_np,\n",
    "        analytical_sol_fn(x_eval_np),\n",
    "        label=f\"Analytic solution\",\n",
    "        color=\"green\",\n",
    "        alpha=0.75,\n",
    "    )\n",
    "    ax.set(title=\"Logistic equation solved with NNs\", xlabel=\"t\", ylabel=\"f(t)\")\n",
    "    ax.legend()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.semilogy(loss_evolution)\n",
    "    ax.set(title=\"Loss evolution\", xlabel=\"# epochs\", ylabel=\"Loss\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
